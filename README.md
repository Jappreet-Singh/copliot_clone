# ğŸš€ AI Chatbot with RAG (Angular + FastAPI + Ollama)

A **fully local, free AI chatbot** with **Retrieval-Augmented Generation (RAG)**, similar to ChatGPT, built using Angular, FastAPI, and Ollama.

Runs **entirely on your machine** â€” no paid APIs, no token limits, and no internet required after setup.

---

## ğŸ¯ Project Overview

This project demonstrates a **production-style AI system** using:
- Angular (frontend)
- FastAPI (backend)
- Ollama (local LLM)
- ChromaDB (vector database for RAG)

The chatbot can:
- Answer general questions
- Ingest documents (PDF / TXT)
- Retrieve relevant context from documents
- Generate grounded, context-aware responses

---

## ğŸ“¦ Features

- Real-time AI chat
- Streaming responses (token-by-token)
- **Retrieval-Augmented Generation (RAG)**
- Document upload (PDF / TXT)
- Local vector database (ChromaDB)
- Local embeddings via Ollama
- No API keys or usage limits
- FastAPI backend with streaming
- Angular chat UI
- Modular and extensible architecture

---

## ğŸ› ï¸ Tech Stack

### Frontend
- Angular 20
- TypeScript
- HttpClient

### Backend
- FastAPI (Python 3.11)
- Uvicorn

### AI & RAG
- Ollama (local LLM)
- LangChain
- ChromaDB
- Ollama Embeddings

### Supported Models
- `llama3`
- `phi3`
- `mistral`
- `qwen2`
- `deepseek-coder`

---

## ğŸ§  How RAG Works

1. User uploads a document (PDF or TXT)
2. Backend extracts text
3. Text is chunked and embedded
4. Embeddings are stored in ChromaDB
5. User asks a question
6. Relevant chunks are retrieved via similarity search
7. Retrieved context is injected into the system prompt
8. Ollama generates a grounded response

---

## ğŸ“¥ Installation & Setup

### 1ï¸âƒ£ Install Ollama

Download: https://ollama.com/download

### ğŸ“¥ Model Setup

Pull a model:
```bash
ollama pull llama3 

test model:
ollama run llama3

### Backend Setup(FastApi)

- python -m venv venv venv\Scripts\activate
- pip install fastapi uvicorn ollama \
  langchain langchain-ollama langchain-chroma \
  chromadb pymupdf python-multipart

test backend:
uvicorn chatApi:app --reload

### Frontend Setup(Angular)

- npm install
- npg serve
-should run it on http://localhost:4200 because of CORS

## ğŸ“¡ API Endpoints

### POST /message

Streaming AI chat response.

Request
{
  "message": "Explain the uploaded document"
}

Response
(streamed text/plain)

### POST /uploadfile

Upload a document for RAG ingestion.

Supported formats
.pdf
.txt

Response

{
  "filename": "example.pdf",
  "summary": "High-level document summary..."
}

## ğŸ“ Project Structure
  /src/
  â””â”€ app/
  â”‚  â”‚  â”œâ”€ backend/
  â”‚  â”‚  â”‚  â”œâ”€ chatApi.py
  â”‚  â”‚  â”‚  â”œâ”€ chroma_db/
  â”‚  â”‚  â”‚  â”œâ”€ uploads/
  â”‚  â”‚  â”‚  â”œâ”€ upload_file/
  â”‚  â”‚  â”‚  â”‚ â””â”€ upload_file.py
  â”‚  â”‚  â”‚  â””â”€ rag/
  â”‚  â”‚  â”‚     â”œâ”€ ingest.py
  â”‚  â”‚  â”‚     â”œâ”€ query.py
  â”‚  â”‚  â”‚     â””â”€ vectorStorage.py
  â”‚  â”‚  â”œâ”€ chat/
  â”‚  â”‚  â”‚  â”œâ”€ chat.ts
  â”‚  â”‚  â”‚  â”œâ”€ chat.html
  â”‚  â”‚  â”‚  â””â”€ chat.css
  â”‚  â”‚  â”œâ”€  app.config.ts
  â”‚  â”‚  â”œâ”€ app.css
  â”‚  â”‚  â”œâ”€ app.html
  â”‚  â”‚  â”œâ”€ app.routes.ts
  â”‚  â”‚  â”œâ”€ app.spec.ts
  â”‚  â”‚  â””â”€ app.ts
  â”‚  â”œâ”€ index.html
  â”‚  â”œâ”€ main.ts
  â”‚  â””â”€ styles.css
  â”œâ”€ .editorconfig
  â”œâ”€ .gitignore
  â”œâ”€ angular.json
  â”œâ”€ package.json
  â”œâ”€ package-lock.json
  â”œâ”€ README.md
  â”œâ”€ tsconfig.app.json
  â”œâ”€ tsconfig.json
  â””â”€ tsconfig.spec.json
      
      
## ğŸ§ª Example Usage

Upload a document:
company_policy.pdf

Ask:
â€œWhat does the document say about vacation policy?â€

The AI responds using retrieved document context.

## ğŸ”§ Configuration

Change the model in the backend:
model = "llama3"

Other supported models:
phi3
mistral
qwen2
deepseek-coder

## ğŸ§± Completed Phases

Phase 1: Angular UI & FastAPI setup

Phase 2: Local LLM integration with Ollama

Phase 3: RAG system with document ingestion and vector search 